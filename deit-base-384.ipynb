{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7045881,"sourceType":"datasetVersion","datasetId":4054446}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, \nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision import transforms as T # for simplifying the transforms\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, sampler, random_split\nfrom torchvision import models","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Data Visualization\nimport plotly.express as px","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom tqdm import tqdm\nimport time\nimport copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_classes(data_dir):\n    all_data = datasets.ImageFolder(data_dir)\n    return all_data.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport os\n\n# Define an image validation function\ndef is_valid_image(file_path):\n    try:\n        with Image.open(file_path) as img:\n            img.verify()  # This will raise an exception if the image is invalid\n        return True\n    except Exception as e:\n        return False\n\ndef get_data_loaders(data_dir, batch_size, train=False):\n    if train:\n        # Train\n        transform = transforms.Compose([\n            # Data augmentation for training, add as needed\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomApply([transforms.ColorJitter()], p=0.25),\n            transforms.Resize(438),\n            transforms.CenterCrop(384),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n        train_data = datasets.ImageFolder(os.path.join(data_dir, \"/kaggle/input/all-dataset2-splitted/ALL_Splitted/train/\"), transform=transform)\n        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n        return train_loader, len(train_data)\n    else:\n        # Validation and Test\n        transform = transforms.Compose([\n            transforms.Resize(438),\n            transforms.CenterCrop(384),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n        val_data = datasets.ImageFolder(os.path.join(data_dir, \"/kaggle/input/all-dataset2-splitted/ALL_Splitted/val/\"), transform=transform)\n        test_data = datasets.ImageFolder(os.path.join(data_dir, \"/kaggle/input/all-dataset2-splitted/ALL_Splitted/test/\"), transform=transform)\n\n        # Iterate through the dataset and remove corrupted images\n        val_data.samples = [(image, label) for image, label in val_data.samples if is_valid_image(image)]\n        test_data.samples = [(image, label) for image, label in test_data.samples if is_valid_image(image)]\n\n        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n\n        return val_loader, test_loader, len(val_data), len(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_loaders(data_dir, batch_size, train = False):\n    if train:\n        #train\n        transform = T.Compose([\n            #T.RandomHorizontalFlip(),\n            #T.RandomVerticalFlip(),\n            #T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n            T.Resize(438),\n            T.CenterCrop(384),\n            T.ToTensor(),\n            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n           # T.RandomErasing(p=0.2, value='random')\n        ])\n        train_data = datasets.ImageFolder(os.path.join(data_dir, \"/kaggle/input/all-dataset2-splitted/ALL_Splitted/train/\"), transform = transform)\n        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n        return train_loader, len(train_data)\n    else:\n        # val/test\n        transform = T.Compose([ # We dont need augmentation for test transforms\n            T.Resize(438),\n            T.CenterCrop(384),\n            T.ToTensor(),\n            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n        ])\n        val_data = datasets.ImageFolder(os.path.join(data_dir, \"/kaggle/input/all-dataset2-splitted/ALL_Splitted/val/\"), transform=transform)\n        test_data = datasets.ImageFolder(os.path.join(data_dir, \"/kaggle/input/all-dataset2-splitted/ALL_Splitted/val/\"), transform=transform)\n        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n        return val_loader, test_loader, len(val_data), len(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/all-dataset2-splitted/ALL_Splitted/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train_loader, train_data_len) = get_data_loaders(dataset_path, 128, train=True)\n(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loaders(dataset_path, 32, train=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = get_classes(\"/kaggle/input/all-dataset2-splitted/ALL_Splitted/val/\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path1= \"/kaggle/input/all-dataset2-splitted/ALL_Splitted/val/\"\n\n# Get Class Names\nclass_names = sorted(os.listdir(dataset_path1))\nn_classes = len(class_names)\n\n# Show\nprint(f\"Class Names : {class_names}\")\nprint(f\"Number of Classes  : {n_classes}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dataset_path =\"/kaggle/input/all-dataset2-splitted/ALL_Splitted/val/\"\n# Calculate class distribution\nclass_dis = [len(os.listdir(pred_dataset_path + name)) for name in class_names]\n\n# Visualization\nfig = px.pie(names=class_names, values=class_dis, title=\"Prediction Class Distribution\")\nfig.update_layout({'title':{'x':0.45}})\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloaders = {\n    \"train\": train_loader,\n    \"val\": val_loader\n}\ndataset_sizes = {\n    \"train\": train_data_len,\n    \"val\": valid_data_len\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_loader), len(val_loader), len(test_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data_len, valid_data_len, test_data_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now, for the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Now, we import timm, torchvision image models\n!pip install timm # kaggle doesnt have it installed by default\nimport timm\nmodel = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_384', pretrained=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloaders = {\n    \"train\": train_loader,\n    \"val\": val_loader\n}\ndataset_sizes = {\n    \"train\": train_data_len,\n    \"val\": valid_data_len\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nn_inputs = model.head.in_features\nmodel.head = nn.Sequential(\n    nn.Linear(n_inputs, 2048),\n    nn.BatchNorm1d(2048),  # Adding batch normalization for regularization\n    nn.ReLU(inplace=True),\n    nn.Dropout(0.5),  # Increasing dropout rate for stronger regularization\n    nn.Linear(2048, 1024),\n    nn.BatchNorm1d(1024),\n    nn.ReLU(inplace=True),\n    nn.Dropout(0.5),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),\n    nn.ReLU(inplace=True),\n    nn.Dropout(0.5),\n    nn.Linear(512, len(classes))\n)\nmodel = model.to(device)\nprint(model.head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion =  nn.CrossEntropyLoss()\ncriterion = criterion.to(device)\noptimizer = optim.Adam(model.head.parameters(), lr=0.001)\n\n# lr scheduler\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, f1_score, classification_report, cohen_kappa_score, roc_auc_score, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport time\nimport copy\nimport torch\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, f1_score, classification_report, cohen_kappa_score, roc_auc_score, roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns\n\ndef plot_roc_curve(true_labels, pred_probs, classes):\n    # Convert true labels to one-hot encoded form\n    true_labels_onehot = label_binarize(true_labels, classes=np.unique(true_labels))\n\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(len(classes)):\n        fpr[i], tpr[i], _ = roc_curve(true_labels_onehot[:, i], pred_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(true_labels_onehot.ravel(), pred_probs.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # Compute macro-average ROC curve and ROC area\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classes))]))\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(len(classes)):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n    mean_tpr /= len(classes)\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot ROC curve\n    plt.figure(figsize=(10, 8))\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='Micro-average (area = {0:0.2f})'.format(roc_auc[\"micro\"]), linestyle=':', linewidth=4)\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='Macro-average (area = {0:0.2f})'.format(roc_auc[\"macro\"]), linestyle=':', linewidth=4)\n    for i in range(len(classes)):\n        plt.plot(fpr[i], tpr[i], label='{0} (area = {1:0.2f})'.format(classes[i], roc_auc[i]))\n    plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC)')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=100, patience=300):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    no_improvement_count = 0\n\n    val_true_labels = []\n    val_pred_labels = []\n    val_pred_probs = []\n    train_acc_history = []\n    val_acc_history = []\n    train_loss_history = []\n    val_loss_history = []\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print(\"-\" * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0.0\n\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    try:\n                        outputs = model(inputs)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n\n                        if phase == 'train':\n                            loss.backward()\n                            optimizer.step()\n\n                        running_loss += loss.item() * inputs.size(0)\n                        running_corrects += torch.sum(preds == labels.data)\n\n                        if phase == 'val':\n                            val_true_labels += labels.tolist()\n                            val_pred_labels += preds.tolist()\n                            val_pred_probs += torch.softmax(outputs, dim=1).tolist()\n                    except OSError as e:\n                        print(f\"Error processing an image: {str(e)}\")\n                        continue\n\n            if phase == 'train':\n                train_loss_history.append(running_loss / dataset_sizes[phase])\n            else:\n                val_loss_history.append(running_loss / dataset_sizes[phase])\n\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, running_loss / dataset_sizes[phase], epoch_acc))\n\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n                no_improvement_count = 0\n            elif phase == 'val':\n                no_improvement_count += 1\n\n            if phase == 'train':\n                train_acc_history.append(epoch_acc.item())\n            else:\n                val_acc_history.append(epoch_acc.item())\n\n        print()\n\n        if no_improvement_count >= patience:\n            print(f\"No improvement in validation accuracy for {no_improvement_count} epochs. Early stopping...\")\n            break\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n\n    model.load_state_dict(best_model_wts)\n\n    # Calculate confusion matrix, F1 score, sensitivity, specificity, accuracy, and Cohen's kappa\n    confusion_mat = confusion_matrix(val_true_labels, val_pred_labels)\n    f1 = f1_score(val_true_labels, val_pred_labels, average='weighted')\n    sensitivity = confusion_mat.diagonal() / confusion_mat.sum(axis=1)\n    specificity = np.diag(confusion_mat) / np.sum(confusion_mat, axis=1)\n    accuracy = np.sum(np.diag(confusion_mat)) / np.sum(confusion_mat)\n    true_positives = np.diag(confusion_mat)\n    true_negatives = np.sum(confusion_mat) - (np.sum(true_positives) + np.sum(confusion_mat.sum(axis=0)) - np.sum(true_positives))\n    false_positives = confusion_mat.sum(axis=0) - true_positives\n    false_negatives = confusion_mat.sum(axis=1) - true_positives\n    kappa = cohen_kappa_score(val_true_labels, val_pred_labels)\n\n    # Print confusion matrix, F1 score, sensitivity, specificity, accuracy, and Cohen's kappa\n    print(\"Confusion Matrix:\")\n    print(confusion_mat)\n    print(\"F1 Score: {:.4f}\".format(f1))\n    print(\"Sensitivity (Recall):\", sensitivity)\n    print(\"Specificity:\", specificity)\n    print(\"Accuracy: {:.4f}\".format(accuracy))\n    print(\"True Positives:\", true_positives)\n    print(\"True Negatives:\", true_negatives)\n    print(\"False Positives:\", false_positives)\n    print(\"False Negatives:\", false_negatives)\n    print(\"Cohen's Kappa:\", kappa)\n\n    # Convert true labels and predicted labels to one-hot encoded form for multiclass AUC calculation\n    val_true_labels_onehot = label_binarize(val_true_labels, classes=np.unique(val_true_labels))\n    val_pred_probs = np.array(val_pred_probs)\n\n    # Calculate AUC for each class\n    auc_scores = roc_auc_score(val_true_labels_onehot, val_pred_probs, average=None)\n    # Calculate macro-average AUC\n    macro_auc = roc_auc_score(val_true_labels_onehot, val_pred_probs, average='macro')\n\n    print(\"AUC Scores (per class):\", auc_scores)\n    print(\"Macro-average AUC:\", macro_auc)\n\n    # Print classification report\n    target_names = [str(i) for i in range(len(classes))]\n    print(classification_report(val_true_labels, val_pred_labels, target_names=target_names))\n\n    # Plot accuracy curves\n    plt.plot(range(1, len(train_acc_history) + 1), train_acc_history, label='Train')\n    plt.plot(range(1, len(val_acc_history) + 1), val_acc_history, label='Validation')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n    plt.show()\n\n    # Plot loss curves\n    plt.plot(range(1, len(train_loss_history) + 1), train_loss_history, label='Train')\n    plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, label='Validation')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.show()\n\n    # Plot ROC curve\n    plot_roc_curve(val_true_labels, val_pred_probs, classes)\n    # Calculate confusion matrix\n    confusion_mat = confusion_matrix(val_true_labels, val_pred_labels, labels=np.arange(len(classes)))\n    # Plot colored confusion matrix\n    plt.figure(figsize=(15, 15))\n    sns.heatmap(confusion_mat, annot=True, fmt=\".0f\", cmap=\"Blues\")\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    # Set ticks and tick labels\n    tick_labels = classes\n    plt.xticks(np.arange(len(classes)) + 0.5, tick_labels, rotation=90, ha='right')\n    plt.yticks(np.arange(len(classes)) + 0.5, tick_labels,rotation=360, ha='right')\n    plt.show()\n\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler)\n\nexample = torch.rand(1, 3, 384, 384)\ntraced_script_module = torch.jit.trace(model.cpu(), example)\ntraced_script_module.save(\"/kaggle/working/deit384_100epochs.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nmodel_path = \"/kaggle/working/deit384_100epochs.pt\"\nmodel = torch.jit.load(model_path)\nmodel = model.to(device)\n\ntest_loss = 0.0\nclass_correct = list(0 for i in range(len(classes)))\nclass_total = list(0 for i in range(len(classes)))\nmodel.eval()\n\nfor data, target in tqdm(test_loader):\n    data, target = data.to(device), target.to(device)\n    with torch.no_grad(): # turn off autograd for faster testing\n        output = model(data)\n        loss = criterion(output, target)\n    test_loss = loss.item() * data.size(0)\n    _, pred = torch.max(output, 1)\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    if len(target) == 32:\n        for i in range(32):\n            label = target.data[i]\n            class_correct[label] += correct[i].item()\n            class_total[label] += 1\n\ntest_loss = test_loss / test_data_len\n\nprint('Test Loss: {:.{}f}'.format(test_loss, max(15, -int(np.log10(test_loss)) + 3)))\n\nfor i in range(len(classes)):\n    if class_total[i] > 0:\n        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n        ))\n    else:\n        print(\"Test accuracy of %5s: NA\" % (classes[i]))\nprint('\\nTest Accuracy : {:.4f} ({}/{})'.format(\n      100. * np.sum(class_correct) / np.sum(class_total),\n      np.sum(class_correct), np.sum(class_total)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.preprocessing import label_binarize\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Collect the true labels and predicted probabilities from the test set\ntest_true_labels = []\ntest_pred_probs = []\n\nmodel.eval()  # Set model to evaluation mode\nfor data, target in tqdm(test_loader):\n    data, target = data.to(device), target.to(device)\n    \n    with torch.no_grad():\n        output = model(data)\n        probs = torch.softmax(output, dim=1)  # Get the predicted probabilities\n        test_pred_probs.extend(probs.cpu().numpy())  # Store the predicted probabilities\n        test_true_labels.extend(target.cpu().numpy())  # Store the true labels\n\n# Convert the true labels and predicted probabilities to numpy arrays\ntest_true_labels = np.array(test_true_labels)\ntest_pred_probs = np.array(test_pred_probs)\n\n# Convert true labels to one-hot encoding format (needed for precision-recall calculation)\ntest_true_labels_onehot = label_binarize(test_true_labels, classes=np.arange(len(classes)))\n\n# Function to plot PR curve\ndef plot_pr_curve(true_labels, pred_probs, classes):\n    precision = dict()\n    recall = dict()\n    average_precision = dict()\n    \n    # Calculate Precision-Recall and plot curve for each class\n    for i in range(len(classes)):\n        precision[i], recall[i], _ = precision_recall_curve(true_labels[:, i], pred_probs[:, i])\n        average_precision[i] = average_precision_score(true_labels[:, i], pred_probs[:, i])\n\n    # Compute micro-average PR curve and average precision\n    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(true_labels.ravel(), pred_probs.ravel())\n    average_precision[\"micro\"] = average_precision_score(true_labels, pred_probs, average=\"micro\")\n\n    # Plot PR curve\n    plt.figure(figsize=(10, 8))\n    plt.plot(recall[\"micro\"], precision[\"micro\"], label='Micro-average (AP = {0:0.2f})'.format(average_precision[\"micro\"]), linestyle=':', linewidth=4)\n\n    for i in range(len(classes)):\n        plt.plot(recall[i], precision[i], label='{0} (AP = {1:0.2f})'.format(classes[i], average_precision[i]))\n\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall Curve')\n    plt.legend(loc=\"lower left\")\n    plt.show()\n\n# Call the function to plot PR curve\nplot_pr_curve(test_true_labels_onehot, test_pred_probs, classes)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}